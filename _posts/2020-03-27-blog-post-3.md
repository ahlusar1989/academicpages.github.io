---
title: 'Derivation of Support Vector Machines'
date: 2020-03-27
permalink: /posts/2020/03/blog-post-3/

Recently, I am experimenting with different mechanisms for deriving decision functions for multiclass classification problems. I have spent some time reading and experiementing with examples the following references:


1. Marti A. Hearst. 1998. Support Vector Machines. IEEE Intelligent Systems 13, 4 (July 1998), 18â€“28. DOI:https://doi.org/10.1109/5254.708428
2. James, Gareth & Witten, Daniela & Hastie, Trevor & Tibshirani, Robert. (2013). An Introduction to Statistical Learning: With Applications in R. 

In light of this, I have contrived a few illustrative examples and derivations:


\documentstyle[12pt]{article}
Support Vector Machines (SVM) {#support-vector-machines-svm .unnumbered .unnumbered}
=============================

Linear separation of a feature space {#linear-separation-of-a-feature-space .unnumbered .unnumbered}
------------------------------------

A hyper plane in an n-D feature space can be represented by the
following equation:
$$f({\bf x})={\bf x}^T {\bf w}+b=\sum_{i=1}^n x_i w_i+b=0$$ Dividing by
$||{\bf w}||$, we get
$$\frac{{\bf x}^T {\bf w}}{||{\bf w}||}=P_{\bf w}({\bf x})
=-\frac{b}{||{\bf w}||}$$ indicating that the projection of any point
${\bf x}$ on the plane onto the vector ${\bf w}$ is always
$-b/||{\bf w}||$, i.e., ${\bf w}$ is the normal direction of the plane,
and $|b|/||{\bf w}||$ is the distance from the origin to the plane. Note
that the equation of the hyper plane is not unique. $c\,f({\bf x})=0$
represents the same plane for any $c$.

The n-D space is partitioned into two regions by the plane.
Specifically, we define a mapping function
$y=sign(f({\bf x})) \in \{1,-1\}$,
$$f({\bf x})={\bf x}^T {\bf w}+b=\left\{ \begin{array}{ll} >0, & 
  y=sign(f({\bf x}))=1,\;{\bf x}\in P  \\
  <0, &  y=sign(f({\bf x}))=-1,\;{\bf x}\in N \\
\end{array} \right.$$ Any point ${\bf x}\in P$ on the positive side of
the plane is mapped to 1, while any point ${\bf x}\in N$ on the negative
side is mapped to -1. A point ${\bf x}$ of unknown class will be
classified to P if $f({\bf x})>0$, or N if $f({\bf x})<0$.

**Example:**

A straight line in 2D space ${\bf x}=[x_1, x_2]^T$ described by the
following equation: $$f({\bf x})={\bf x}^T {\bf w}+b=[x_1,x_2]
\left[ \begin{array}{c} w_1 \\ w_2 \end{array} \right]+b
=[x_1,x_2]\left[ \begin{array}{c} 1 \\ 2 \end{array} \right]-1
=x_1+2x_2-1=0$$ devides the 2D plane into two halves. The distance
between the origin and the line is
$$\frac{|b|}{||{\bf w}||}=\frac{1}{\sqrt{w_1^2+w_2^2}}=\frac{1}{\sqrt{5}}=0.447$$
Consider three points:

-   ${\bf x}_0=[0.5,\;0.25]^T$, $f({\bf x}_0)=0.5+2\times 0.25-1=0$,
    i.e., ${\bf x}_0$ is on the plane;

-   ${\bf x}_1=[1,\;0.25]^T$, $f({\bf x}_1)=1+2\times 0.25-1=0.5>0$,
    i.e., ${\bf x}_1$ is above the straight line;

-   ${\bf x}_2=[0.5,\;0]^T$, $f({\bf x}_2)=0.5+2\times 0-1=-0.5<0$,
    i.e., ${\bf x}_2$ is below the straight line.

The learning problem {#the-learning-problem .unnumbered .unnumbered}
--------------------

Given a set $K$ training samples from two linearly separable classes P
and N: $$\{ ({\bf x}_k, y_k), k=1,\cdots,K \}$$ where $y_k \in \{1,-1\}$
labels ${\bf x}_k$ to belong to either of the two classes. we want to
find a hyper-plane in terms of ${\bf w}$ and $b$, that linearly
separates the two classes.

Before the classifier is properly trained, the actual output
$y'=sign(f({\bf x}))$ may not be the same as the desired output $y$.
There are four possible cases:

$$\begin{tabular}{c|l|l|l} \hline
& Input $({\bf x},y)$ & Output $y'=sign(f({\bf x}))$ & result   \\ \hline \hline
1 & $({\bf x},y= 1)$ & $y'= 1=  y $ & corrrect  \\ \hline
2 & $({\bf x},y=-1)$ & $y'= 1\ne y$ & incorrect \\ \hline
3 & $({\bf x},y= 1)$ & $y'=-1\ne y$ & incorrect \\ \hline
4 & $({\bf x},y=-1)$ & $y'=-1=  y $ & corrrect  \\ \hline
\end{tabular}$$

The weight vector ${\bf w}$ is updated whenever the result is incorrect
(mistake driven):

-   If $({\bf x},y=-1)$ but $y'=1\ne y$ (case 2 above), then
    $${\bf x}^{new}={\bf w}^{old}+\eta y {\bf x}={\bf w}^{old}-\eta {\bf x}$$
    When the same ${\bf x}$ is presented again, we have
    $$f({\bf x})={\bf x}^T{\bf w}^{new}+b
      ={\bf x}^T{\bf w}^{old}-\eta {\bf x}^T{\bf x}+b<{\bf x}^T{\bf w}^{old}+b$$
    The output $y'=sign(f({\bf x}))$ is more likely to be $y=-1$ as
    desired. Here $0 < \eta < 1$ is the learning rate.

-   If $({\bf x},y=1)$ but $y'=-1\ne y$ (case 3 above), then
    $${\bf w}^{new}={\bf w}^{old}+\eta y {\bf x}={\bf w}^{old}+\eta {\bf x}$$
    When the same ${\bf x}$ is presented again, we have
    $$f({\bf x})={\bf x}^T{\bf w}^{new}+b={\bf x}^T{\bf w}^{old}+\eta {\bf x}^T{\bf x}+b>{\bf x}^T{\bf w}^{old}+b$$
    The output $y'=sign(f({\bf x}))$ is more likely to be $y=1$ as
    desired.

Summarizing the two cases:
$$\mbox{if} \;\;y f({\bf x})= y ({\bf x}^T{\bf w}^{old}+b)<0,\;\;\mbox{then}\;\;
     {\bf w}^{new}={\bf w}^{old}+\eta y {\bf x}$$ The two correct cases
(cases 1 and 4) can also be summarized as
$$y f({\bf x})= y ({\bf x}^T {\bf w}+b)\ge 0$$ which is the condition a
successful classifier should satisfy.

We assume initially ${\bf w}=0$, and the $K$ training samples are
presented repeatedly, the training will yield:
$${\bf w}=\sum_{i=1}^K \alpha_i y_i {\bf x}_i$$ where $\alpha_i>0$. Note
that ${\bf w}$ is expressed as a linear combination of the training
samples. After receiving a new sample $({\bf x}_i,y_i)$, vector
${\bf w}$ is updated by $$\begin{aligned}
  && \mbox{if} \;\;y_i f({\bf x}_i)=y_i ({\bf x}_i^T{\bf w}^{old}+b)
  =y_i\left(\sum_{j=1}^m \alpha_j y_j({\bf x}_i^T{\bf x}_j)+b\right)<0,
  \nonumber \\
  && \mbox{then}\;\; {\bf w}^{new}={\bf w}^{old}+\eta y_i {\bf x}_i
  =\sum_{j=1}^m \alpha_j y_j {\bf x}_j+\eta y_i {\bf x}_i,\;\;\;\mbox{i.e.}\;\;\;
  \alpha_i^{new}=\alpha_i^{old}+\eta    \nonumber\end{aligned}$$ Now
both the decision function
$$f({\bf x})={\bf x}^T {\bf w}+b=\sum_{j=1}^m \alpha_j y_j ({\bf x}^T {\bf x}_j)+b$$
and the learning law
$$\mbox{if} \;\;y_i\left(\sum_{j=1}^m \alpha_j y_j({\bf x}_i^T{\bf x}_j)+b\right)<0,
\;\;\;  \mbox{then}\;\; \alpha_i^{new}=\alpha_i^{old}+\eta$$ are
expressed in terms of the inner production of input vectors.

SVM Dereivations {#svm-dereivations .unnumbered .unnumbered}
----------------

For a decision hyper-plane ${\bf x}^T {\bf w}+b=0$ to separate the two
classes $P=\{({\bf x}_i,1)\}$ and $N=\{({\bf x}_i,-1)\}$, it has to
satisfy $$y_i ({\bf x}_i^T{\bf w}+b) \ge 0$$ for both ${\bf x}_i \in P$
and ${\bf x}_i \in N$. Among all such planes satisfying this condition,
we want to find the optimal one $H_0$ that separates the two classes
with the maximal margin (the distance between the decision plane and the
closest sample points).

The optimal plane should be in the middle of the two classes, so that
the distance from the plane to the closest point on either side is the
same. We define two additional planes $H_+$ and $H_-$ that are parallel
to $H_0$ and go through the point closest to the plane on either side:
$${\bf x}^T {\bf w}+b=1,\;\;\;\;\mbox{and}\;\;\;\;{\bf x}^T {\bf w}+b=-1$$
All points ${\bf x}_i \in P$ on the positive side should satisfy
$${\bf x}_i^T {\bf w}+b \ge 1,\;\;\;\;y_i=1$$ and all points
${\bf x}_i \in N$ on the negative side should satisfy
$${\bf x}_i^T {\bf w}+b \le -1,\;\;\;\; y_i=-1$$ These can be combined
into one inequality:
$$y_i ({\bf x}_i^T{\bf w} +b) \ge 1,\;\;\;(i=1,\cdots,m)$$ The equality
holds for those points on the planes $H_+$ or $H_-$. Such points are
called *support vectors*, for which $${\bf x}_i^T {\bf w}+b = y_i$$
i.e., the following holds for all support vectors:
$$b=y_i-{\bf x}_i^T {\bf w}=y_i-\sum_{j=1}^m  \alpha_j y_j ({\bf x}_i^T {\bf x}_j)$$

Moreover, the distances from the origin to the three parallel planes
$H_-$, $H_0$ and $H_+$ are, respectively, $|b-1|/||{\bf w}||$,
$|b|/||{\bf w}||$, and $|b+1|/||{\bf w}||$, and the distance between
planes $H_-$ and $H_+$ is $2/||{\bf w}||$.

Our goal is to maximize this distance, or, equivalantly, to minimize the
norm $||{\bf w}||$. Now the problem of finding the optimal decision
plane in terms of ${\bf w}$ and $b$ can be formulated as:
$$\begin{aligned}
  & \mbox{minimize} & \frac{1}{2}{\bf w}^T {\bf w}=\frac{1}{2}||{\bf w}||^2 
  \;\;\;\;\;\;\mbox{(objective function)}   \nonumber \\
  & \mbox{subject to} & y_i ({\bf x}_i^T {\bf w}+b) \ge 1,\;\;\mbox{or}\;\;
  1-y_i ({\bf x}_i^T {\bf w}+b) \le 0,\;\;\;\;(i=1,\cdots,m)
  \nonumber\end{aligned}$$ Since the objective function is quadratic,
this constrained optimization problem is called a quadratic program (QP)
problem. (If the objective function is linear instead, the problem is a
linear program (LP) problem). This QP problem can be solved by Lagrange
multipliers method to minimize the following
$$L_p({\bf w},b,{\bf \alpha})=\frac{1}{2}||{\bf w}||^2
+\sum_{i=1}^m \alpha_i(1-y_i({\bf x}_i^T{\bf w}+b))$$ with respect to
${\bf w}$, $b$ and the Lagrange coefficients $\alpha_i\ge 0$
$(i=1,\cdots,\alpha_m)$. We let
$$\frac{\partial}{\partial W}L_p({\bf w},b)=0,\;\;\;
    \frac{\partial}{\partial b}L_p({\bf w},b)=0$$ These lead,
respectively, to
$${\bf w}=\sum_{j=1}^m \alpha_j y_j {\bf x}_j,\;\;\;\mbox{and}\;\;\;\;
    \sum_{i=1}^m \alpha_i y_i=0$$ Substituting these two equations back
into the expression of $L({\bf w},b)$, we get the *dual problem* (with
respect to $\alpha_i$) of the above *primal problem*: $$\begin{aligned}
&   \mbox{maximize} & L_d({\bf \alpha})=
    \sum_{i=1}^m\alpha_i -\frac{1}{2}
    \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j {\bf x}_i^T,{\bf x}_j
    \nonumber \\
&   \mbox{subject to} & \alpha_i\ge 0,\;\;\;\;
    \sum_{i=1}^m \alpha_i y_i=0 \nonumber\end{aligned}$$ The dual
problem is related to the primal problem by:
$$L_d({\bf \alpha})=inf_{({\bf w},b)} L_p({\bf w},b,{\bf \alpha})$$
i.e., $L_d$ is the greatest lower bound (infimum) of $L_p$ for all
${\bf w}$ and $b$.

Solving this dual problem (an easier problem than the primal one), we
get $\alpha_i$, from which ${\bf w}$ of the optimal plane can be found.

Those points ${\bf x}_i$ on either of the two planes $H_+$ and $H_-$
(for which the equality $y_i({\bf w}^T {\bf x}_i+b)=1$ holds) are called
*support vectors* and they correspond to positive Lagrange multipliers
$\alpha_i>0$. The training depends only on the support vectors, while
all other samples away from the planes $H_+$ and $H_-$ are not
important.

For a support vector ${\bf x}_i$ (on the $H_-$ or $H_+$ plane), the
constraining condition is
$$y_i \left({\bf x}_i^T {\bf w}+b\right) = 1\;\;\;\;(i \in sv)$$ here
$sv$ is a set of all indices of support vectors ${\bf x}_i$
(corresponding to $\alpha_i > 0$). Substituting
$${\bf w}=\sum_{j=1}^m \alpha_j y_j {\bf x}_j=\sum_{j\in sv} \alpha_j y_j {\bf x}_j$$
we get $$y_i(\sum_{j\in sv} \alpha_j y_j {\bf x}^T_i {\bf x}_j+b) = 1$$
Note that the summation only contains terms corresponding to those
support vectors ${\bf x}_j$ with $\alpha_j>0$, i.e.
$$y_i \sum_{j\in sv} \alpha_j y_j {\bf x}^T_i {\bf x}_j= 1-y_i b$$ For
the optimal weight vector ${\bf w}$ and optimal $b$, we have:
$$\begin{aligned}
||{\bf w}||^2&=&{\bf w}^T {\bf w}=\sum_{i\in sv} \alpha_i y_i {\bf x}^T_i
    \sum_{j\in sv} \alpha_j y_j {\bf x}_j   =
    \sum_{i\in sv} \alpha_i y_i 
    \sum_{j\in sv} \alpha_j y_j {\bf x}^T_i{\bf x}_j    \nonumber \\
&=& \sum_{i\in sv} \alpha_i (1-y_i b)
    =\sum_{i\in sv} \alpha_i - b\sum_{i\in sv} \alpha_i y_i \nonumber \\
&=& \sum_{i\in sv} \alpha_i \nonumber \end{aligned}$$ The last equality
is due to $\sum_{i=1}^m \alpha_i y_i=0$ shown above. Recall that the
distance between the two margin planes $H_+$ and $H_-$ is
$2/||{\bf w}||$, and the margin, the distance between $H_+$ (or $H_-$)
and the optimal decision plane $H_0$, is
$$\frac{1}{||{\bf w}||}=\left(\sum_{i\in sv} \alpha_i\right)^{-1/2}$$

Soft Margin SVM {#soft-margin-svm .unnumbered .unnumbered}
---------------

When the two classes are not linearly separable (e.g., due to noise),
the condition for the optimal hyper-plane can be relaxed by including an
extra term:
$$y_i ({\bf x}_i^T{\bf w} +b) \ge 1-\xi_i,\;\;\;(i=1,\cdots,m)$$ For
minimum error, $\xi_i \ge 0$ should be minimized as well as
$||{\bf w}||$, and the objective function becomes: $$\begin{aligned}
&   \mbox{minimize} & {\bf w}^T {\bf w}+C\sum_{i=1}^m \xi_i^k
    \nonumber \\
&   \mbox{subject to} & y_i ({\bf x}_i^T {\bf w}+b) \ge 1-\xi_i,
    \;\;\;\mbox{and}\;\;\;\xi_i \ge 0;\;\;\;(i=1,\cdots,m)
    \nonumber\end{aligned}$$ Here $C$ is a regularization parameter that
controls the trade-off between maximizing the margin and minimizing the
training error. Small C tends to emphasize the margin while ignoring the
outliers in the training data, while large C may tend to overfit the
training data.

When $k=2$, it is called 2-norm soft margin problem: $$\begin{aligned}
&   \mbox{minimize}   & {\bf w}^T {\bf w}+C\sum_{i=1}^m \xi_i^2
    \nonumber \\
&   \mbox{subject to} & y_i ({\bf x}_i^T {\bf w}+b) \ge 1-\xi_i,\;\;\;(i=1,\cdots,m)
    \nonumber \\\end{aligned}$$ Note that the condition $\xi_i \ge 0$ is
dropped, as if $\xi_i<0$, we can set it to zero and the objective
function is further reduced.) Alternatively, if we let $k=1$, the
problem can be formulated as $$\begin{aligned}
&   \mbox{minimize}     & {\bf w}^T {\bf w}+C\sum_{i=1}^m \xi_i
    \nonumber \\
&   \mbox{subject to}   & y_i ({\bf x}_i^T {\bf w}+b) \ge 1-\xi_i
    \;\;\;\mbox{and}\;\;\;\xi_i \ge 0;\;\;\;(i=1,\cdots,m)
    \nonumber \\\end{aligned}$$ This is called 1-norm soft margin
problem. The algorithm based on 1-norm setup, when compared to 2-norm
algorithm, is less sensitive to outliers in training data. When the data
is noisy, 1-norm method should be used to ignore the outliers.

### L2-Norm Soft Margin {#l2-norm-soft-margin .unnumbered .unnumbered}

The primal Lagrangian for 2-norm problem above is
$$L_p({\bf w},b,\xi,\alpha)=\frac{1}{2}{\bf w}^T{\bf w}+\frac{C}{2}\sum_{i=1}^m\xi_i^2
    -\sum_{i=1}^m \alpha_i[y_i({\bf w}^T{\bf x}+b)-1+\xi_i]$$
Substituting
$$\frac{\partial L}{\partial {\bf w}}={\bf w}-\sum_{i=1}^m y_i\alpha_i{\bf x}_i=0;
    \;\;\;
\frac{\partial L}{\partial \xi}=C\xi-\alpha=0;
    \;\;\;
\frac{\partial L}{\partial b}=\sum_{i=1}^m y_i\alpha_i=0$$ into the
primal Lagrangian, we get the dual problem $$\begin{aligned}
&   \mbox{maximize} & L_d(\alpha)=\sum_{i=1}^m\alpha_i
    -\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m y_iy_j\alpha_i\alpha_j {\bf x}_j^T{\bf x}_i
    -\frac{1}{2C}\sum_{i=1}^m \alpha_i^2
    \nonumber \\
&&  =\sum_{i=1}^m\alpha_i
    -\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m y_iy_j\alpha_i\alpha_j({\bf x}_j^T{\bf x}_i
    +\frac{1}{C}\delta_{ij})
    \nonumber \\
&   \mbox{subject to}   & \alpha_i \ge 0,\;\;\;\;
    \sum_{i=1}^m \alpha_i y_i=0 \nonumber\end{aligned}$$ This QP program
can be solved for $\alpha_i$. All support vectors ${\bf x}_i$
corresponding to $\alpha_i > 0$ satisfy:
$$y_i({\bf x}_i^T{\bf w}+b)=1-\xi_i$$ Substituting
${\bf w}=\sum_{j\in sv} y_j\alpha_j{\bf x}_j$ into this equation, we get
$$y_i(\sum_{j\in sv} y_j\alpha_j({\bf x}_i^T {\bf x}_j)+b)=1-\xi_i,
    \;\;\;\;\mbox{i.e.,}\;\;\;\;
    y_i\sum_{j\in sv} y_j\alpha_j({\bf x}_i^T {\bf x}_j)=1-\xi_i-y_ib$$
For the optimal weight ${\bf w}$, we have $$\begin{aligned}
||{\bf w}||^2&=&{\bf w}^T {\bf w}
    =\sum_{i\in sv} \alpha_i y_i {\bf x}^T_i \sum_{j\in sv} \alpha_j y_j {\bf x}_j
    =\sum_{i\in sv} \alpha_i y_i \sum_{j\in sv} \alpha_j y_j {\bf x}^T_i{\bf x}_j
    \nonumber \\
&=& \sum_{i\in sv} \alpha_i (1-\xi_i-y_i b)
    =\sum_{i\in sv} \alpha_i-\sum_{i\in sv}\alpha_i \xi_i
    -b\sum_{i\in sv} y_i \alpha_i   \nonumber \\
&=& \sum_{i\in sv} \alpha_i-\sum_{i\in sv}\alpha_i \xi_i
    =\sum_{i\in sv} \alpha_i -\frac{1}{C}\sum_{i\in sv}\alpha_i^2
        \nonumber \end{aligned}$$ The last equation is due to
$\xi_i=\alpha_i/C$. The optimal margin is
$$1/||{\bf w}||=(\sum_{i\in sv} \alpha_i -\frac{1}{C}\sum_{i\in sv}\alpha_i^2)^{-1/2}$$

### L1-Norm Soft Margin {#l1-norm-soft-margin .unnumbered .unnumbered}

The primal Lagrangian for 1-norm problem above is
$$L_p({\bf w},b,\xi,\alpha,\gamma)=\frac{1}{2}{\bf w}^T{\bf w}+C\sum_{i=1}^m\xi_i
    -\sum_{i=1}^m \alpha_i[y_i({\bf w}^T{\bf x}+b)-1+\xi_i]-\sum_{i=1}^m\gamma_i\xi_i$$
with $\alpha_i \ge 0$ and $\gamma_i \ge 0$. Substituting
$$\frac{\partial L}{\partial {\bf w}}={\bf w}-\sum_{i=1}^m y_i\alpha_i{\bf x}_i=0;
    \;\;\;
\frac{\partial L}{\partial \xi}=C-\alpha_i-\gamma_i=0;
    \;\;\;
\frac{\partial L}{\partial b}=\sum_{i=1}^m y_i\alpha_i=0$$ into the
primal Lagrangian, we get the dual problem $$\begin{aligned}
&   \mbox{maximize} & L_d(\alpha,\gamma)=\sum_{i=1}^m\alpha_i
    -\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m y_iy_j\alpha_i\alpha_j {\bf x}_j^T{\bf x}_i
    -\sum_{i=1}^m \alpha_i\xi_i-\sum_{i=1}^m \gamma_i\xi_i
    +C\sum_{i=1}^m\xi_i
    \nonumber \\
&&  =\sum_{i=1}^m\alpha_i
    -\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m y_iy_j\alpha_i\alpha_j {\bf x}_j^T{\bf x}_i
    \nonumber \\
&   \mbox{subject to}   & 0 \le \alpha_i \le C,\;\;\;\;
    \sum_{i=1}^m \alpha_i y_i=0 \nonumber\end{aligned}$$ Note that
interestingly the objective function of the dual problem is identical to
that of the linearly separable problem discussed previously, due to the
nice cancellation based on $C=\alpha_i+\gamma_i$. Also, since
$\alpha_i \ge 0$ and $\gamma_i \ge 0$, we have $0 \le \alpha_i \le C$.
Solving this QP problem for $\alpha_i$, we get the optimal decision
plane ${\bf w}$ and $b$ with the margin
$$(\sum_{i\in sv}\sum_{j\in sv} \alpha_i\alpha_jy_iy_j{\bf x}_i^T{\bf x}_j)^{-1/2}$$

Kernel Mapping {#kernel-mapping .unnumbered .unnumbered}
--------------

The algorithm above converges only for linearly separable data. If the
data set is not linearly separable, we can map the samples ${\bf x}$
into a feature space of higher dimensions:
$${\bf x} \longrightarrow \phi({\bf x})$$ in which the classes can be
linearly separated. The decision function in the new space becomes:
$$f({\bf x})=\phi({\bf x})^T {\bf w}+b=
\sum_{j=1}^m \alpha_j y_j (\phi({\bf x})^T \phi({\bf x}_j))+b$$ where
$${\bf w}=\sum_{j=1}^m \alpha_j y_j \phi({\bf x}_j)$$ and $b$ are the
parameters of the decision plane in the new space. As the vectors
${\bf x}_i$ appear only in inner products in both the decision function
and the mapping function $\phi({\bf x})$ does not need to be explicitly
specified. Instead, all we need is the inner product of the vectors in
the new space. The function $\phi({\bf x})$ is a kernel-induced
*implicit* mapping.

**Definition:** A kernel is a function that takes two vectors
${\bf x}_i$ and ${\bf x}_j$ as arguments and returns the value of the
inner product of their images $\phi({\bf x}_i)$ and $\phi({\bf x}_j)$:
$$K({\bf x}_1,{\bf x}_2)=\phi({\bf x}_1)^T\phi({\bf x}_2)$$ As only the
inner product of the two vectors in the new space is returned, the
dimensionality of the new space is not important.

The learning algorithm in the kernel space can be obtained by replacing
all inner products in the learning algorithm in the original space with
the kernels:
$$f({\bf x})=\phi({\bf x})^T {\bf w}+b=\sum_{j=1}^m \alpha_j y_j K({\bf x},{\bf x}_j)+b$$
The parameter $b$ can be found from any support vectors ${\bf x}_i$:
$$b=y_i-\phi({\bf x}_i)^T {\bf w}=y_i-\sum_{j=1}^m  \alpha_j y_j 
(\phi({\bf x}_i)^T \phi({\bf x}_j))=y_i-\sum_{j=1}^m  \alpha_j y_j K({\bf x}_i,{\bf x}_j)$$

**Example 0:** linear kernel

Assume ${\bf x}=[x_1,\cdots,x_n]^T$, ${\bf z}=[z_1,\cdots,z_n]^T$,

$$K({\bf x},{\bf z})={\bf x}^T {\bf z}\sum_{i=1}^n x_1z_1$$

**Example 1:** polynomial kernels

Assume ${\bf x}=[x_1,x_2]^T$, ${\bf z}=[z_1,z_2]^T$, $$\begin{aligned}
    K({\bf x},{\bf z})=({\bf x}^T{\bf z})^2&=&(x_1z_1+x_2z_2)^2
        =x_1^2z_1^2+x_2^2z_2^2+2x_1z_1x_2z_2
    \nonumber \\
    &=&<(x_1^2,x_2^2,\sqrt{2}x_1x_2),(z_1^2,z_2^2,\sqrt{2}z_1z_2)>
    =\phi({\bf x})^T \phi({\bf z})
    \nonumber\end{aligned}$$ This is a mapping from a 2-D space to a 3-D
space. The order can be changed from 2 to general d.

<!-- **Example 2:**
$$K({\bf x},{\bf z})=e^{-||{\bf x}-{\bf z}||^2/2\sigma^2}$$

**Example 3:**
$$K({\bf x},{\bf z})=K({\bf x},{\bf z})K({\bf x},{\bf x})^{-1/2}K({\bf z},{\bf z})^{-1/2}$$ -->
